{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 6 IDL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnsaPZT_-f1Z",
        "colab_type": "text"
      },
      "source": [
        "GROUP MEMBERS :\n",
        "1.  Aniruddh Shukla \n",
        "2. Gaurav Singhal \n",
        "3. Himanshi Bajaj "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12hujeqdxAuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ivYQKcYxUpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJp6qS0dzcHL",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7CuEqtZzbrC",
        "colab_type": "code",
        "outputId": "52d19c4c-26d4-4a5f-c6ab-415e5223a2fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python prepare_data2.py shakespeare_input.txt skp \\\\n\\\\n+ -m 500"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-31 19:04:10.573778: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 31022 sequences...\n",
            "Longest sequence is 3094 characters. If this seems unreasonable, consider using the maxlen argument!\n",
            "Removing sequences longer than 500 characters...\n",
            "29429 sequences remaining.\n",
            "Longest remaining sequence has length 499.\n",
            "Removing length-0 sequences...\n",
            "29429 sequences remaining.\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n",
            "Serialized 23000 sequences...\n",
            "Serialized 23100 sequences...\n",
            "Serialized 23200 sequences...\n",
            "Serialized 23300 sequences...\n",
            "Serialized 23400 sequences...\n",
            "Serialized 23500 sequences...\n",
            "Serialized 23600 sequences...\n",
            "Serialized 23700 sequences...\n",
            "Serialized 23800 sequences...\n",
            "Serialized 23900 sequences...\n",
            "Serialized 24000 sequences...\n",
            "Serialized 24100 sequences...\n",
            "Serialized 24200 sequences...\n",
            "Serialized 24300 sequences...\n",
            "Serialized 24400 sequences...\n",
            "Serialized 24500 sequences...\n",
            "Serialized 24600 sequences...\n",
            "Serialized 24700 sequences...\n",
            "Serialized 24800 sequences...\n",
            "Serialized 24900 sequences...\n",
            "Serialized 25000 sequences...\n",
            "Serialized 25100 sequences...\n",
            "Serialized 25200 sequences...\n",
            "Serialized 25300 sequences...\n",
            "Serialized 25400 sequences...\n",
            "Serialized 25500 sequences...\n",
            "Serialized 25600 sequences...\n",
            "Serialized 25700 sequences...\n",
            "Serialized 25800 sequences...\n",
            "Serialized 25900 sequences...\n",
            "Serialized 26000 sequences...\n",
            "Serialized 26100 sequences...\n",
            "Serialized 26200 sequences...\n",
            "Serialized 26300 sequences...\n",
            "Serialized 26400 sequences...\n",
            "Serialized 26500 sequences...\n",
            "Serialized 26600 sequences...\n",
            "Serialized 26700 sequences...\n",
            "Serialized 26800 sequences...\n",
            "Serialized 26900 sequences...\n",
            "Serialized 27000 sequences...\n",
            "Serialized 27100 sequences...\n",
            "Serialized 27200 sequences...\n",
            "Serialized 27300 sequences...\n",
            "Serialized 27400 sequences...\n",
            "Serialized 27500 sequences...\n",
            "Serialized 27600 sequences...\n",
            "Serialized 27700 sequences...\n",
            "Serialized 27800 sequences...\n",
            "Serialized 27900 sequences...\n",
            "Serialized 28000 sequences...\n",
            "Serialized 28100 sequences...\n",
            "Serialized 28200 sequences...\n",
            "Serialized 28300 sequences...\n",
            "Serialized 28400 sequences...\n",
            "Serialized 28500 sequences...\n",
            "Serialized 28600 sequences...\n",
            "Serialized 28700 sequences...\n",
            "Serialized 28800 sequences...\n",
            "Serialized 28900 sequences...\n",
            "Serialized 29000 sequences...\n",
            "Serialized 29100 sequences...\n",
            "Serialized 29200 sequences...\n",
            "Serialized 29300 sequences...\n",
            "Serialized 29400 sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b8b864h1E_9",
        "colab_type": "code",
        "outputId": "695dd66a-9a59-4bbe-b24a-baa24b498b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from prepare_data2 import parse_seq\n",
        "import pickle\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"skp.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data = data.map(lambda x: parse_seq(x))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"skp_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "\n",
        "print(vocab)\n",
        "print(vocab_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'u': 3, 'f': 4, 'A': 5, 'L': 6, 'E': 7, 'q': 8, '?': 9, 'M': 10, 'I': 11, ';': 12, 'x': 13, 'X': 14, 'd': 15, '3': 16, 'F': 17, 'T': 18, 'g': 19, 'r': 20, '&': 21, 'm': 22, 'S': 23, 'y': 24, 'b': 25, '$': 26, ']': 27, 'N': 28, ',': 29, 'Z': 30, '.': 31, 'H': 32, 'c': 33, 'z': 34, 'B': 35, 'p': 36, \"'\": 37, 'Q': 38, ' ': 39, 'n': 40, 'Y': 41, 'i': 42, 'V': 43, 'P': 44, '-': 45, 'v': 46, 'R': 47, 'J': 48, '\\n': 49, 'K': 50, 'h': 51, 'o': 52, 'W': 53, '!': 54, 'w': 55, 'G': 56, 'e': 57, 'O': 58, 'k': 59, '[': 60, 'a': 61, 'l': 62, 'j': 63, 'U': 64, 'C': 65, 't': 66, 'D': 67, 's': 68, ':': 69, '<PAD>': 0, '<S>': 1, '</S>': 2}\n",
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEeifrgC4XDM",
        "colab_type": "code",
        "outputId": "0876d5ce-7b75-4f3e-e658-45e3d48bd005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for item in data.take(5):\n",
        "    to_chars = \"\".join(ind_to_ch[ch] for ch in item.numpy())\n",
        "    print(to_chars)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<S>First Citizen:\n",
            "Before we proceed any further, hear me speak.</S>\n",
            "<S>All:\n",
            "Speak, speak.</S>\n",
            "<S>First Citizen:\n",
            "You are all resolved rather to die than to famish?</S>\n",
            "<S>All:\n",
            "Resolved. resolved.</S>\n",
            "<S>First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.</S>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PODBl-0uGkOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating input sequences\n",
        "def split_input(chunk):\n",
        "  input_data = chunk[:-1]\n",
        "  return input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ITzJUP3SjFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating ouput sequences\n",
        "def split_target(chunk):\n",
        "  output_data = chunk[1:]\n",
        "  return output_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3CP8eWOJdOU",
        "colab_type": "code",
        "outputId": "77960078-2c9f-4dd5-f9e3-063af537c133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "dataset_in = data.map(split_input)\n",
        "dataset_out = data.map(split_target)\n",
        "for input_example in dataset_in.take(1):\n",
        "  print(\"Input example: \" , \"\".join(ind_to_ch[ch] for ch in input_example.numpy()))\n",
        "for output_example in dataset_out.take(1):\n",
        "  print(\"Output example: \" , \"\".join(ind_to_ch[ch] for ch in output_example.numpy()))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input example:  <S>First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "Output example:  First Citizen:\n",
            "Before we proceed any further, hear me speak.</S>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nTR1cpxf659",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoding(x):\n",
        "  return tf.one_hot(x, depth = vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImUWT7w90NOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "dataset_in = dataset_in.padded_batch(batch_size=BATCH_SIZE,drop_remainder=True,padding_values=0,padded_shapes=500)\n",
        "one_hot_encoded_data_in = dataset_in.map(one_hot_encoding)\n",
        "dataset_out = dataset_out.padded_batch(batch_size=BATCH_SIZE,drop_remainder=True,padding_values=0,padded_shapes=500)\n",
        "# one_hot_encoded_data_out = dataset_out.map(one_hot_encoding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeH6Rfr8YXqO",
        "colab_type": "text"
      },
      "source": [
        "# Testing with one batch, masked and unmasked loss\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqEUn5NwOdgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using pre-built RNN and a dense layer of size - vocab\n",
        "rnn_units = 512\n",
        "def build_model(vocab_size, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                  tf.keras.layers.GRU(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "                  tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1dKyKhRZzZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  building the model\n",
        "model = build_model(vocab_size=vocab_size,rnn_units=rnn_units,batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AqjMw8VJwrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  calculating loss function\n",
        "def loss_function(logits,labels):\n",
        "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avNNgFYOr3v6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def masking(unmasked_batch):\n",
        "  # calculating non zeros count for each sequence in a batch\n",
        " nonzero_count  = tf.math.count_nonzero(unmasked_batch,axis=1,dtype= tf.float32)\n",
        " # subtracting as we didn't consider last char of input \n",
        " nonzero_count = nonzero_count - 1\n",
        " #  converting mask into a 2D tensor of size batch x time_step\n",
        " padding_withzeros = tf.sequence_mask(nonzero_count,maxlen=500,dtype=tf.float32)\n",
        " return padding_withzeros,nonzero_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfLhNGcBZ9M9",
        "colab_type": "code",
        "outputId": "ca4706dc-285d-48ee-b6f9-e4465586c646",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "for input_example_batch,output_example_batch,input_batch in zip(one_hot_encoded_data_in.take(1),dataset_out.take(1),dataset_in.take(1)):\n",
        "  #  getting predicted output for one batch\n",
        "  example_batch_predits = model(input_example_batch)\n",
        "\n",
        "  padding_withzeros,nonzero_count = masking(input_batch)\n",
        "  # calculating loss for each time_step, then summing the all time_steps and finally averaging for each sequence in a batch wrt length (without masking)\n",
        "  loss = loss_function(example_batch_predits,output_example_batch)\n",
        "  summed_loss_without_masking_per_batch = tf.reduce_sum(loss,axis=1)\n",
        "  average_loss_without_masking_per_batch = tf.divide(summed_loss_without_masking_per_batch,nonzero_count)\n",
        "  print(\"Average loss without masking : \\n\",average_loss_without_masking_per_batch)\n",
        "  \n",
        "  masked_loss = loss * padding_withzeros\n",
        "  # summing the all time_steps and finally averaging for each sequence in a batch wrt length (with masking)\n",
        "  summed_loss_per_batch = tf.reduce_sum(masked_loss,axis=1)\n",
        "  average_loss_per_batch = tf.divide(summed_loss_per_batch,nonzero_count)\n",
        "\n",
        "  print(\"Average loss with masking : \\n\",average_loss_per_batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss without masking : \n",
            " tf.Tensor(\n",
            "[ 34.920418  116.23101    32.23315    87.18817    28.324173   80.49564\n",
            "  24.6617     38.784447   52.342175   31.275099   36.11722    29.517317\n",
            "  17.637384   44.55022     8.10941    18.093128    9.531103  130.74515\n",
            "  58.148605   26.534275   31.742018   18.908773    8.969084   23.29816\n",
            "  39.52266     5.402406    9.3985     16.025812    4.765436   39.5198\n",
            "   6.2365894   8.637399   29.11315    23.29606    80.49222    24.960316\n",
            "  16.663986   59.80728    59.80882    10.678351   40.27895     5.926048\n",
            "  41.889015    6.3447933  16.53184    48.692524   25.567835    4.343719\n",
            "  12.0167675  67.516884    6.97226    80.48905    51.061      63.433388\n",
            "  63.429325   39.521706   20.567757   23.82962    12.9771805  59.81504\n",
            "  13.823136   29.107405   58.15937    14.790729   19.792433   90.97706\n",
            "  23.303484  348.58047    87.20034   149.43492    12.441825   41.070007\n",
            "  87.19303    37.407494   58.15153    69.769875   37.408966   61.577564\n",
            "  27.580091   12.296082    6.684065   19.61564    12.155244   15.11235\n",
            " 110.12423    18.094915   22.078444    7.179767    9.487434    8.01934\n",
            "  83.703964   56.580223   87.19352    83.70856   149.41423    35.514324\n",
            "   6.9002814  36.120033   38.7892      5.0143056  44.555832    8.019191\n",
            "  41.064972   27.952696   51.06537    95.11819    51.072384   15.329934\n",
            "  41.892223   23.302761    4.653045   59.821484   52.3526     83.70937\n",
            "  20.36846    41.887604   80.49386    67.528244   20.567213   19.98055\n",
            "  21.40345    80.50097    37.4126      8.138872   33.79683    24.960915\n",
            "  44.555237   27.227175 ], shape=(128,), dtype=float32)\n",
            "Average loss with masking : \n",
            " tf.Tensor(\n",
            "[4.2516522 4.2437387 4.2438726 4.24476   4.248655  4.2529736 4.242928\n",
            " 4.243206  4.248412  4.2472067 4.2467213 4.247233  4.2478013 4.2423577\n",
            " 4.248873  4.2490425 4.251603  4.236159  4.2439184 4.2472196 4.242028\n",
            " 4.2524967 4.252889  4.2460527 4.250884  4.250152  4.2453833 4.245718\n",
            " 4.2484703 4.247575  4.2502947 4.2494316 4.2519073 4.2441745 4.246825\n",
            " 4.2483144 4.250394  4.245525  4.248247  4.245607  4.247492  4.250662\n",
            " 4.2483063 4.247575  4.248602  4.246099  4.249506  4.2495008 4.249511\n",
            " 4.2438784 4.253008  4.244644  4.243459  4.2498775 4.244625  4.250484\n",
            " 4.249605  4.250008  4.2511854 4.2522836 4.248341  4.247212  4.2565517\n",
            " 4.2467985 4.2478414 4.246913  4.252001  4.2500544 4.2586236 4.255259\n",
            " 4.250788  4.2507744 4.24894   4.2493715 4.247573  4.251127  4.2511063\n",
            " 4.2583084 4.24845   4.249896  4.2489285 4.2554173 4.2502856 4.2508564\n",
            " 4.2499714 4.2508106 4.249785  4.2493496 4.2504387 4.250506  4.2459364\n",
            " 4.2485156 4.25117   4.2511773 4.2402353 4.253752  4.2487793 4.24954\n",
            " 4.249414  4.2528477 4.248747  4.25041   4.247523  4.254587  4.2462173\n",
            " 4.2526035 4.252848  4.2485023 4.253272  4.250969  4.2495236 4.260135\n",
            " 4.2595806 4.2495136 4.2490716 4.2490325 4.253258  4.2585163 4.248802\n",
            " 4.2479777 4.2480206 4.2561464 4.254417  4.2473726 4.2516394 4.249786\n",
            " 4.246896  4.2523603], shape=(128,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKZdrKLKa4hM",
        "colab_type": "code",
        "outputId": "5d355336-a4c5-4675-8059-2527adf9f617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_18 (GRU)                 multiple                  897024    \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             multiple                  35910     \n",
            "=================================================================\n",
            "Total params: 932,934\n",
            "Trainable params: 932,934\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B_rBsEOiX_o",
        "colab_type": "code",
        "outputId": "a875b596-2e17-434c-b36b-61cc48a1175b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# printing output for the first batch\n",
        "sample_indices = tf.random.categorical(example_batch_predits[0],num_samples=1)\n",
        "sample_indices = tf.squeeze(sample_indices,axis=-1).numpy()\n",
        "sample_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([66, 17,  9, 63,  3, 26, 51, 69, 53, 57, 27, 54,  1, 40, 23, 59, 60,\n",
              "       24, 48,  4, 54, 24,  9, 15, 51, 24,  2, 29, 39, 51,  2, 68, 58, 48,\n",
              "       42, 66, 60, 34, 65, 24, 46, 69, 35, 56, 47, 46,  1, 17, 13, 18, 43,\n",
              "       58, 32, 16, 66, 11, 29, 33,  6, 42, 13, 31, 62, 27, 33,  0, 20, 36,\n",
              "       59, 28, 57, 64, 45, 25, 17,  1, 34, 14,  6,  0, 59, 43, 33,  1, 68,\n",
              "       13,  5, 28, 26, 37,  1, 26,  3,  8,  1, 58, 29, 48, 17,  1, 52,  4,\n",
              "       48, 56, 15,  4, 66, 18, 68,  1, 27, 48, 31, 12, 14, 18, 50,  7, 48,\n",
              "       65, 21, 49,  4,  1, 17, 35, 69, 15, 44, 39, 30, 27, 13, 45, 40, 50,\n",
              "       15, 32, 50,  2, 58, 69, 63,  4, 58, 52, 28, 28, 61, 20, 30, 13, 19,\n",
              "       32, 66, 63, 26, 47, 68, 43, 14, 43, 37, 17, 46, 48, 47, 37, 57, 56,\n",
              "       27, 32, 25, 57, 49, 44, 25,  6, 22, 57, 21, 32, 41, 54, 42, 32, 48,\n",
              "       41, 39, 21, 66, 17,  4, 69, 34, 41,  4, 39, 19, 56, 31, 46, 16, 61,\n",
              "       51, 66, 16, 59, 52,  4, 53, 24,  5, 54,  8, 69, 22, 55, 54, 18,  7,\n",
              "        6, 38, 11,  0, 61, 40,  4, 19, 50, 34,  2,  4, 58, 17,  4, 42,  2,\n",
              "       23,  7,  6, 10, 56, 31, 63,  1, 58, 37, 53, 69, 51, 41, 14, 56, 17,\n",
              "       51,  4,  9, 28, 23, 27, 27, 23, 11, 21, 43, 64, 13, 19, 41, 55, 23,\n",
              "       57, 41, 68, 25, 41, 35, 52, 56, 61, 31,  6, 15, 11, 19, 37,  6, 18,\n",
              "       18, 15, 34, 36, 30,  3, 14,  0,  1, 37,  7, 45, 34, 19, 63, 58, 22,\n",
              "       19, 63, 43, 57, 42, 57, 12, 19, 67,  9, 12, 41, 17,  4, 19,  2, 48,\n",
              "       11, 61, 10, 34, 45, 32, 21, 21, 67, 42, 39, 66, 20, 26, 17, 65, 11,\n",
              "       58, 24, 55, 52, 35, 59,  1, 54, 30, 14, 20, 13, 65, 67, 56, 38, 28,\n",
              "       41, 32, 19,  4, 54, 65,  4, 61, 44,  0, 68, 16, 44, 21, 68, 56, 59,\n",
              "       53, 43, 34, 18, 57, 54, 46, 42, 32, 50, 12, 67, 54,  9, 34,  2, 38,\n",
              "       25, 66, 68, 14, 69, 57,  2, 11, 15, 19, 44, 22, 58, 44,  6, 42, 62,\n",
              "       37, 55, 47, 64, 38, 32, 28, 17, 41, 51, 35, 15, 14, 53, 18, 34, 67,\n",
              "       61, 61, 28, 65, 26, 67, 25, 31, 17, 64, 41, 68, 17,  9, 45, 40, 67,\n",
              "        0, 17, 23, 51, 46, 55, 19, 61, 33, 17, 11, 29, 67,  4, 48, 60, 31,\n",
              "       28, 62, 29, 27, 51, 66, 45, 19, 48, 17, 36, 51, 36, 62,  3, 59, 24,\n",
              "       56,  9, 30, 13, 62, 35, 42, 55, 58, 22, 60, 18, 30,  6, 24, 12, 57,\n",
              "       45, 56, 51, 15, 67, 19, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVsAV5QIk5kH",
        "colab_type": "code",
        "outputId": "5604812a-386a-42f4-89fc-9fc3d1ff3fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "for input_example_batch in dataset_in.take(1):\n",
        "  print(\"Input : \\n \" , \"\".join(ind_to_ch[ch] for ch in input_example_batch[0].numpy()))\n",
        "  print(\" \\n Next character predicted: \\n\" , \"\".join(ind_to_ch[ch] for ch in sample_indices))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : \n",
            "  <S>First Citizen:\n",
            "Before we proceed any further, hear me speak.<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            " \n",
            " Next character predicted: \n",
            " tF?ju$h:We]!<S>nSk[yJf!y?dhy</S>, h</S>sOJit[zCyv:BGRv<S>FxTVOH3tI,cLix.l]c<PAD>rpkNeU-bF<S>zXL<PAD>kVc<S>sxAN$'<S>$uq<S>O,JF<S>ofJGdftTs<S>]J.;XTKEJC&\n",
            "f<S>FB:dP Z]x-nKdHK</S>O:jfOoNNarZxgHtj$RsVXV'FvJR'eG]Hbe\n",
            "PbLme&HY!iHJY &tFf:zYf gG.v3aht3kofWyA!q:mw!TELQI<PAD>anfgKz</S>fOFfi</S>SELMG.j<S>O'W:hYXGFhf?NS]]SI&VUxgYwSeYsbYBoGa.LdIg'LTTdzpZuX<PAD><S>'E-zgjOmgjVeie;gD?;YFfg</S>JIaMz-H&&Di tr$FCIOywoBk<S>!ZXrxCDGQNYHgf!CfaP<PAD>s3P&sGkWVzTe!viHK;D!?z</S>QbtsX:e</S>IdgPmOPLil'wRUQHNFYhBdXWTzDaaNC$Db.FUYsF?-nD<PAD>FShvwgacFI,DfJ[.Nl,]ht-gJFphplukyG?ZxlBiwOm[TZLy;e-GhdDgX\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FTeYOwHRbzS",
        "colab_type": "text"
      },
      "source": [
        "# Implementing the Complete RNN for entire dataset and sampling with keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztgZDJzlXC7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating ouput sequences\n",
        "def split_target(chunk):\n",
        "  output_data = chunk[1:]\n",
        "  return output_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-PbWGJGXDIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset_in = data.map(split_input)\n",
        "dataset_in = data\n",
        "dataset_out = data.map(split_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvzJ4lsJXDS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoding(x):\n",
        "  return tf.one_hot(x, depth = vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCHOY6YiXSMV",
        "colab_type": "code",
        "outputId": "545d5973-ff66-487f-8110-73166de8fec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "dataset_in = dataset_in.padded_batch(batch_size=BATCH_SIZE,drop_remainder=True,padding_values=0,padded_shapes=500)\n",
        "one_hot_encoded_data_in = dataset_in.map(one_hot_encoding)\n",
        "dataset_out = dataset_out.padded_batch(batch_size=BATCH_SIZE,drop_remainder=True,padding_values=0,padded_shapes=500)\n",
        "for x in one_hot_encoded_data_in:\n",
        "  print(x.shape)\n",
        "  break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 500, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwEPaHAPmCIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using pre-built RNN and a dense layer of size - vocab\n",
        "rnn_units = 1024\n",
        "def build_model(vocab_size, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                  tf.keras.layers.GRU(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "                  tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJgM65PLSRnZ",
        "colab_type": "code",
        "outputId": "e5030bea-9885-447a-faea-97c9753f7beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#  building the model\n",
        "model = build_model(vocab_size=vocab_size,rnn_units=rnn_units,batch_size=BATCH_SIZE)\n",
        "input_tensor = tf.Variable(tf.initializers.GlorotUniform(seed = 0)(shape=[128, 500, vocab_size]))\n",
        "prediction = model(input_tensor)\n",
        "prediction.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([128, 500, 70])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFzB71P4szrs",
        "colab_type": "code",
        "outputId": "d7acaf86-9bfc-4ddd-a57e-ced676451b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_19 (GRU)                 multiple                  3366912   \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             multiple                  71750     \n",
            "=================================================================\n",
            "Total params: 3,438,662\n",
            "Trainable params: 3,438,662\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PMI4P-wSXEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  calculating loss function\n",
        "def loss_function(logits,labels):\n",
        "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6boN5CEOSeDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def masking(unmasked_batch):\n",
        "  \n",
        "  # calculating non zeros count for each sequence in a batch\n",
        " nonzero_count  = tf.math.count_nonzero(unmasked_batch,axis=1,dtype= tf.float32)\n",
        "\n",
        " # subtracting as we didn't consider last char of input \n",
        " nonzero_count = nonzero_count - 1\n",
        "\n",
        " #  converting mask into a 2D tensor of size batch x time_step\n",
        " padding_withzeros = tf.sequence_mask(nonzero_count,maxlen=500,dtype=tf.float32)\n",
        "\n",
        " return padding_withzeros,nonzero_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzab6wDLkjoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmZ6abrSSmp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epochs = 30\n",
        "def model_execution(Epochs,optimizer,one_hot_encoded_data_in,dataset_out,dataset_in):\n",
        "  for epoch in range(Epochs):\n",
        "    for input_example_batch,output_example_batch,input_batch in zip(one_hot_encoded_data_in,dataset_out,dataset_in):\n",
        "      model_resetting = model.reset_states()\n",
        "      #  getting predicted output for one batch\n",
        "      with tf.GradientTape() as tape:\n",
        "        example_batch_predits = model(input_example_batch)\n",
        "\n",
        "        padding_withzeros,nonzero_count = masking(input_batch)\n",
        "        # calculating loss for each time_step\n",
        "        loss = loss_function(example_batch_predits,output_example_batch)\n",
        "  \n",
        "        masked_loss = loss * padding_withzeros\n",
        "        # summing the all time_steps and finally averaging for each sequence in a batch wrt length (with masking)\n",
        "        summed_loss_per_batch = tf.reduce_sum(masked_loss,axis=1)\n",
        "        average_loss_per_batch = tf.divide(summed_loss_per_batch,nonzero_count)\n",
        "        average_loss = tf.reduce_mean(average_loss_per_batch)\n",
        "\n",
        "      grads = tape.gradient(average_loss_per_batch,model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
        "\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "    print(\"Epoch: {}, Loss: {}\".format(epoch, average_loss))\n",
        "  model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvUvxNO0Wcmb",
        "colab_type": "code",
        "outputId": "1264c376-a5f9-48b3-e56b-5c019886a9b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "learning_rate = 0.001\n",
        "# Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model_execution(Epochs,optimizer,one_hot_encoded_data_in,dataset_out,dataset_in)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 2.2767281532287598\n",
            "Epoch: 1, Loss: 2.06550931930542\n",
            "Epoch: 2, Loss: 1.8966399431228638\n",
            "Epoch: 3, Loss: 1.7667053937911987\n",
            "Epoch: 4, Loss: 1.6627166271209717\n",
            "Epoch: 5, Loss: 1.566677212715149\n",
            "Epoch: 6, Loss: 1.4918911457061768\n",
            "Epoch: 7, Loss: 1.4304687976837158\n",
            "Epoch: 8, Loss: 1.3758869171142578\n",
            "Epoch: 9, Loss: 1.331498622894287\n",
            "Epoch: 10, Loss: 1.2958202362060547\n",
            "Epoch: 11, Loss: 1.2591071128845215\n",
            "Epoch: 12, Loss: 1.2336969375610352\n",
            "Epoch: 13, Loss: 1.2131727933883667\n",
            "Epoch: 14, Loss: 1.1873164176940918\n",
            "Epoch: 15, Loss: 1.1591829061508179\n",
            "Epoch: 16, Loss: 1.134566068649292\n",
            "Epoch: 17, Loss: 1.129105806350708\n",
            "Epoch: 18, Loss: 1.100846529006958\n",
            "Epoch: 19, Loss: 1.0948433876037598\n",
            "Epoch: 20, Loss: 1.0807957649230957\n",
            "Epoch: 21, Loss: 1.0664831399917603\n",
            "Epoch: 22, Loss: 1.0511434078216553\n",
            "Epoch: 23, Loss: 1.044309139251709\n",
            "Epoch: 24, Loss: 1.0253673791885376\n",
            "Epoch: 25, Loss: 1.0155891180038452\n",
            "Epoch: 26, Loss: 1.0001296997070312\n",
            "Epoch: 27, Loss: 0.9842461943626404\n",
            "Epoch: 28, Loss: 0.9772186279296875\n",
            "Epoch: 29, Loss: 0.9690123796463013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMwH2aLjXc7_",
        "colab_type": "code",
        "outputId": "10d71429-e47d-44f6-bd41-b745aec1e7a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_29'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ehqtW0a1Ndp",
        "colab_type": "text"
      },
      "source": [
        "# Applying a Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsPZsEZux64w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generating_language_model = build_model(vocab_size=vocab_size,rnn_units=rnn_units,batch_size=BATCH_SIZE)\n",
        "generating_language_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "generating_language_model.build(tf.TensorShape([1,None,vocab_size]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7BO_OTv0lVS",
        "colab_type": "code",
        "outputId": "9c25b8f0-825e-49f7-e968-3f34f19c4617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "generating_language_model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_2 (GRU)                  multiple                  3366912   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  71750     \n",
            "=================================================================\n",
            "Total params: 3,438,662\n",
            "Trainable params: 3,438,662\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsU7sCDV-wv4",
        "colab_type": "code",
        "outputId": "a8876769-5e19-4af0-a413-5787aba0b574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "ch = 'L'  \n",
        "i = 0\n",
        "for key,val in vocab.items():\n",
        "  if key == ch:\n",
        "    i = val\n",
        "print(i)\n",
        "# convert to one hot vector\n",
        "input_ch = tf.one_hot(i,depth=vocab_size)\n",
        "input_ch = tf.expand_dims(tf.expand_dims(input_ch,axis=0),axis=0)\n",
        "print(input_ch)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "tf.Tensor(\n",
            "[[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0.]]], shape=(1, 1, 70), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-P8Ssee_7_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_function(logits):\n",
        "  return tf.nn.softmax(axis=-1,logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxLg_5xr9_XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "character_index_list = [i]\n",
        "index_list = list(range(vocab_size))\n",
        "generating_language_model.reset_states()\n",
        "for time_step in range(3000):\n",
        "\n",
        "    next_char = generating_language_model(input_ch)\n",
        "    softmax_char = softmax_function(next_char)\n",
        "    softmax_char = softmax_char.numpy()\n",
        "    index = np.random.choice(index_list,p = softmax_char.flatten())\n",
        "    character_index_list.append(index)\n",
        "\n",
        "    input_ch = tf.one_hot(index,depth=vocab_size)\n",
        "    input_ch = tf.expand_dims(tf.expand_dims(input_ch,axis=0),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A88ufkiO-24m",
        "colab_type": "code",
        "outputId": "08a81e5f-d4ec-4953-a7aa-d00481def06c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "seq = [ind_to_ch[ind] for ind in character_index_list]\n",
        "seq = [s.replace('</S>','\\n') for s in seq ]\n",
        "print(\"\".join(seq))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LUCIUS:\n",
            "With that swear how you shall take leave a common man:\n",
            "You seem am I am not I must be in common\n",
            "East from my talen from my sun untided\n",
            "Sir John, for am none redoct!\n",
            "He would venture appeil in my sway,\n",
            "With this intercupes, ducy so ask some take,\n",
            "To hear of made Edmurs us.\n",
            "'Alvise ma!\n",
            "le!\n",
            "\n",
            "has spent it well: or cloud will less than I make\n",
            "The duff to wait upon his mile: be too.\n",
            "Say, God report to these incass, we'll die with thee.\n",
            "\n",
            "Sir, welcome to thee.\n",
            "'Tis sweat, you made us\n",
            "right unto 'em.\n",
            " secret you at a prick,\n",
            "Brought redues you with music.\n",
            " in,\n",
            "Master Antonio, but one that will do.\n",
            "\n",
            "For what we are coming to distime with\n",
            "these the cursus, thy life we have wicked for wife.\n",
            "\n",
            "Where As you shall know for my breast,\n",
            "That put my tongue I was such swaggers to like\n",
            "Enjoy in such a courage and a fear.\n",
            "'s not\n",
            "virtuous, as I live, I have recovery;\n",
            "Bun splean to a maid.\n",
            " not you would,\n",
            "So stoor her borth.\n",
            " but you and in rest,\n",
            "Which I understand them, before he fears and\n",
            "straight to make you both pursuit.\n",
            "Nut both, unain-better. 'Walteve this common!\n",
            "O heavenly mad! Come, my brother ray\n",
            "A pickle of the matter to the matthr'd\n",
            "To do with you.\n",
            " no mother's absence!\n",
            "I am in all I his subpline, and true\n",
            "posence to my love; and so forward, sir,\n",
            "That he be spiot princles; and, follow their full,\n",
            "A mighty sovereign, away.\n",
            " arrew a word.\n",
            " and\n",
            "spiring thee, if it please you, say to my gill.\n",
            "Thou dost ge to publicly friend, sir,\n",
            "Within the field sens bargained sorlower treed.\n",
            " would, part!\n",
            " as\n",
            "I tell my sword: he will not assay a box.\n",
            "\n",
            "\n",
            "Troilus, give me leave; and sinks the life,\n",
            "I'll none of I say. I call her meep.\n",
            " say to:\n",
            "'Tis so, come to his cooglt you say, that that\n",
            "might be brief, thy soul for bears what it is,\n",
            "who say I have enture to me as like\n",
            "Adwass the first: but mine ot love you, which to\n",
            "Absure Pabous, what for mine I call you a\n",
            "whore, nocesy butter to him, and take the cates\n",
            "for thy substing had used mean approve\n",
            "By yielding now as good as thou seemed,\n",
            "And so well and wath, wisely doterides;\n",
            "The proclamation is most wit for a stace,\n",
            "full of colours, bestowed on thy mirth.\n",
            ".\n",
            ", do but poor.\n",
            "\n",
            "And sent you both.\n",
            " weables this resomate\n",
            "Troo such a fairest souble mitth,\n",
            "To be done't for ye clouds, that beg\n",
            "in still it, with a soldier to steal a holdical.\n",
            "\n",
            "ul\n",
            "As I could prove to be thy knightes now\n",
            "In good so long we, was ready; and so swooning\n",
            "Hon so may bless the letter.\n",
            " lock\n",
            "At my heart the field so only troit\n",
            "deeds do the end, sirps outroughbulor. Was ever now\n",
            "his makes with woman's accord but in it?\n",
            "--\n",
            "ur indeed\n",
            "With the labour, to foreland him, front them to\n",
            "me into thee: rascal! I am resembted at suspice.\n",
            "\n",
            "ath\n",
            "Will rul us but, to stend and soft a sort\n",
            "That you within.\n",
            "\n",
            "Good, droppory!\n",
            "Make this dot flyif favour to the mount:\n",
            "That much distraction, says 'em.\n",
            "'\n",
            "less Loon.\n",
            "Peace! thus did recover it, 'twas a ground,\n",
            "And for his niston power you never heard:\n",
            "And I will travolth us, you lift but now\n",
            "I am good an intence, lack you, yet I did\n",
            "at me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_aAxcSVwLWN",
        "colab_type": "text"
      },
      "source": [
        "# Trying King James Bible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGyXhcmLwgyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd67df6d-6629-4765-d7fd-576d242c3a8d"
      },
      "source": [
        "!python prepare_data2.py the-king-james-bible.txt king [0-9]+:[0-9]+  -m 500"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-01 12:10:36.430297: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 31103 sequences...\n",
            "Longest sequence is 533 characters. If this seems unreasonable, consider using the maxlen argument!\n",
            "Removing sequences longer than 500 characters...\n",
            "31102 sequences remaining.\n",
            "Longest remaining sequence has length 448.\n",
            "Removing length-0 sequences...\n",
            "31102 sequences remaining.\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n",
            "Serialized 23000 sequences...\n",
            "Serialized 23100 sequences...\n",
            "Serialized 23200 sequences...\n",
            "Serialized 23300 sequences...\n",
            "Serialized 23400 sequences...\n",
            "Serialized 23500 sequences...\n",
            "Serialized 23600 sequences...\n",
            "Serialized 23700 sequences...\n",
            "Serialized 23800 sequences...\n",
            "Serialized 23900 sequences...\n",
            "Serialized 24000 sequences...\n",
            "Serialized 24100 sequences...\n",
            "Serialized 24200 sequences...\n",
            "Serialized 24300 sequences...\n",
            "Serialized 24400 sequences...\n",
            "Serialized 24500 sequences...\n",
            "Serialized 24600 sequences...\n",
            "Serialized 24700 sequences...\n",
            "Serialized 24800 sequences...\n",
            "Serialized 24900 sequences...\n",
            "Serialized 25000 sequences...\n",
            "Serialized 25100 sequences...\n",
            "Serialized 25200 sequences...\n",
            "Serialized 25300 sequences...\n",
            "Serialized 25400 sequences...\n",
            "Serialized 25500 sequences...\n",
            "Serialized 25600 sequences...\n",
            "Serialized 25700 sequences...\n",
            "Serialized 25800 sequences...\n",
            "Serialized 25900 sequences...\n",
            "Serialized 26000 sequences...\n",
            "Serialized 26100 sequences...\n",
            "Serialized 26200 sequences...\n",
            "Serialized 26300 sequences...\n",
            "Serialized 26400 sequences...\n",
            "Serialized 26500 sequences...\n",
            "Serialized 26600 sequences...\n",
            "Serialized 26700 sequences...\n",
            "Serialized 26800 sequences...\n",
            "Serialized 26900 sequences...\n",
            "Serialized 27000 sequences...\n",
            "Serialized 27100 sequences...\n",
            "Serialized 27200 sequences...\n",
            "Serialized 27300 sequences...\n",
            "Serialized 27400 sequences...\n",
            "Serialized 27500 sequences...\n",
            "Serialized 27600 sequences...\n",
            "Serialized 27700 sequences...\n",
            "Serialized 27800 sequences...\n",
            "Serialized 27900 sequences...\n",
            "Serialized 28000 sequences...\n",
            "Serialized 28100 sequences...\n",
            "Serialized 28200 sequences...\n",
            "Serialized 28300 sequences...\n",
            "Serialized 28400 sequences...\n",
            "Serialized 28500 sequences...\n",
            "Serialized 28600 sequences...\n",
            "Serialized 28700 sequences...\n",
            "Serialized 28800 sequences...\n",
            "Serialized 28900 sequences...\n",
            "Serialized 29000 sequences...\n",
            "Serialized 29100 sequences...\n",
            "Serialized 29200 sequences...\n",
            "Serialized 29300 sequences...\n",
            "Serialized 29400 sequences...\n",
            "Serialized 29500 sequences...\n",
            "Serialized 29600 sequences...\n",
            "Serialized 29700 sequences...\n",
            "Serialized 29800 sequences...\n",
            "Serialized 29900 sequences...\n",
            "Serialized 30000 sequences...\n",
            "Serialized 30100 sequences...\n",
            "Serialized 30200 sequences...\n",
            "Serialized 30300 sequences...\n",
            "Serialized 30400 sequences...\n",
            "Serialized 30500 sequences...\n",
            "Serialized 30600 sequences...\n",
            "Serialized 30700 sequences...\n",
            "Serialized 30800 sequences...\n",
            "Serialized 30900 sequences...\n",
            "Serialized 31000 sequences...\n",
            "Serialized 31100 sequences...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79Bqr1I-xmmf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5218c8fd-f3d3-4ba6-c675-e335c5191478"
      },
      "source": [
        "from prepare_data2 import parse_seq\n",
        "import pickle\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data_k = tf.data.TFRecordDataset(\"king.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data_k = data_k.map(lambda x: parse_seq(x))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab_k = pickle.load(open(\"king_vocab\", mode=\"rb\"))\n",
        "vocab_size_k = len(vocab_k)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab_k.items()}\n",
        "\n",
        "print(vocab_k)\n",
        "print(vocab_size_k)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 3, 'B': 4, '6': 5, 'a': 6, 't': 7, ',': 8, '2': 9, '!': 10, 'y': 11, '?': 12, 'q': 13, '*': 14, 'o': 15, 'z': 16, 'U': 17, 'F': 18, 'M': 19, '1': 20, 'x': 21, 'W': 22, 'v': 23, 'A': 24, ')': 25, ';': 26, '.': 27, 'I': 28, '(': 29, 'g': 30, 'Z': 31, '7': 32, '\\n': 33, 'G': 34, 'd': 35, 'c': 36, 'b': 37, 'h': 38, '3': 39, 's': 40, 'L': 41, '5': 42, 'Q': 43, 'K': 44, 'l': 45, 'r': 46, 'Y': 47, '8': 48, ':': 49, '-': 50, '4': 51, 'O': 52, 'w': 53, 'R': 54, 'V': 55, 'D': 56, 'T': 57, 'C': 58, 'n': 59, 'H': 60, 'E': 61, 'e': 62, 'k': 63, '\\ufeff': 64, 'u': 65, 'p': 66, 'N': 67, 'P': 68, 'J': 69, ' ': 70, '9': 71, '0': 72, \"'\": 73, 'j': 74, 'S': 75, 'f': 76, 'm': 77, '<PAD>': 0, '<S>': 1, '</S>': 2}\n",
            "78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmkJjosTyKE5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "50993d07-2406-4a0f-ff87-b8757db0590f"
      },
      "source": [
        "for item in data_k.take(5):\n",
        "    to_chars = \"\".join(ind_to_ch[ch] for ch in item.numpy())\n",
        "    print(to_chars)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<S>﻿The First Book of Moses:  Called Genesis\n",
            "\n",
            "\n",
            "</S>\n",
            "<S> In the beginning God created the heaven and the earth.\n",
            "\n",
            "</S>\n",
            "<S> And the earth was without form, and void; and darkness was upon\n",
            "the face of the deep. And the Spirit of God moved upon the face of the\n",
            "waters.\n",
            "\n",
            "</S>\n",
            "<S> And God said, Let there be light: and there was light.\n",
            "\n",
            "</S>\n",
            "<S> And God saw the light, that it was good: and God divided the light\n",
            "from the darkness.\n",
            "\n",
            "</S>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_0t87skyPC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating ouput sequences\n",
        "def split_target(chunk):\n",
        "  output_data = chunk[1:]\n",
        "  return output_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhCHQSv9yg5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_in_k = data_k\n",
        "dataset_out_k = data_k.map(split_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTQhtF6nyt5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoding(x):\n",
        "  return tf.one_hot(x, depth = vocab_size_k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obiz9uNXyzp_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48d427b8-744b-4a22-b768-bf01546257e6"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "dataset_in_k = dataset_in_k.padded_batch(batch_size=BATCH_SIZE,drop_remainder=True,padding_values=0,padded_shapes=500)\n",
        "one_hot_encoded_data_in_k = dataset_in_k.map(one_hot_encoding)\n",
        "dataset_out_k = dataset_out_k.padded_batch(batch_size=BATCH_SIZE,drop_remainder=True,padding_values=0,padded_shapes=500)\n",
        "for x in one_hot_encoded_data_in_k:\n",
        "  print(x.shape)\n",
        "  break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 500, 78)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNNRvcX4zKMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using pre-built RNN and a dense layer of size - vocab\n",
        "rnn_units = 1024\n",
        "def build_model(vocab_size_k, rnn_units, batch_size):\n",
        "  model_k = tf.keras.Sequential([\n",
        "                  tf.keras.layers.GRU(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
        "                  tf.keras.layers.Dense(vocab_size_k)\n",
        "  ])\n",
        "  return model_k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpQPtVt8zZ1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "874fc6ac-e6fb-4f53-f7ed-0b5fee194edb"
      },
      "source": [
        "#  building the model\n",
        "model_k = build_model(vocab_size_k=vocab_size_k,rnn_units=rnn_units,batch_size=BATCH_SIZE)\n",
        "input_tensor = tf.Variable(tf.initializers.GlorotUniform(seed = 0)(shape=[128, 500, vocab_size_k]))\n",
        "prediction = model_k(input_tensor)\n",
        "prediction.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([128, 500, 78])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fav20f1Wz4cP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "9dbf2485-da7f-46da-cc04-c27943466ea7"
      },
      "source": [
        "model_k.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    multiple                  3391488   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  79950     \n",
            "=================================================================\n",
            "Total params: 3,471,438\n",
            "Trainable params: 3,471,438\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7LBt5eTz8nE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  calculating loss function\n",
        "def loss_function(logits,labels):\n",
        "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMDBAoYv0GWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def masking(unmasked_batch_k):\n",
        "  \n",
        "  # calculating non zeros count for each sequence in a batch\n",
        " nonzero_count  = tf.math.count_nonzero(unmasked_batch_k,axis=1,dtype= tf.float32)\n",
        "\n",
        " # subtracting as we didn't consider last char of input \n",
        " nonzero_count = nonzero_count - 1\n",
        "\n",
        " #  converting mask into a 2D tensor of size batch x time_step\n",
        " padding_withzeros = tf.sequence_mask(nonzero_count,maxlen=500,dtype=tf.float32)\n",
        "\n",
        " return padding_withzeros,nonzero_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN_Xz2Wc0Poy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints_k'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cLfcX9x0b5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Epochs = 30\n",
        "def model_execution(Epochs,optimizer_k,one_hot_encoded_data_in_k,dataset_out_k,dataset_in_k):\n",
        "  for epoch in range(Epochs):\n",
        "    for input_example_batch_k,output_example_batch_k,input_batch_k in zip(one_hot_encoded_data_in_k,dataset_out_k,dataset_in_k):\n",
        "      model_resetting = model_k.reset_states()\n",
        "      #  getting predicted output for one batch\n",
        "      with tf.GradientTape() as tape:\n",
        "        example_batch_predits_k = model_k(input_example_batch_k)\n",
        "\n",
        "        padding_withzeros,nonzero_count = masking(input_batch_k)\n",
        "        # calculating loss for each time_step\n",
        "        loss = loss_function(example_batch_predits_k,output_example_batch_k)\n",
        "  \n",
        "        masked_loss = loss * padding_withzeros\n",
        "        # summing the all time_steps and finally averaging for each sequence in a batch wrt length (with masking)\n",
        "        summed_loss_per_batch = tf.reduce_sum(masked_loss,axis=1)\n",
        "        average_loss_per_batch = tf.divide(summed_loss_per_batch,nonzero_count)\n",
        "        average_loss = tf.reduce_mean(average_loss_per_batch)\n",
        "\n",
        "      grads = tape.gradient(average_loss_per_batch,model_k.trainable_variables)\n",
        "      optimizer_k.apply_gradients(zip(grads,model_k.trainable_variables))\n",
        "\n",
        "    model_k.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "    print(\"Epoch: {}, Loss: {}\".format(epoch, average_loss))\n",
        "  model_k.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6fPvGGW0hde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "e8224307-e3c7-40b0-f3eb-defc961a7d29"
      },
      "source": [
        "learning_rate = 0.001\n",
        "# Optimizer\n",
        "optimizer_k = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model_execution(Epochs,optimizer_k,one_hot_encoded_data_in_k,dataset_out_k,dataset_in_k)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 1.948115587234497\n",
            "Epoch: 1, Loss: 1.7417001724243164\n",
            "Epoch: 2, Loss: 1.5351203680038452\n",
            "Epoch: 3, Loss: 1.3823587894439697\n",
            "Epoch: 4, Loss: 1.267326831817627\n",
            "Epoch: 5, Loss: 1.193995714187622\n",
            "Epoch: 6, Loss: 1.13751220703125\n",
            "Epoch: 7, Loss: 1.0974154472351074\n",
            "Epoch: 8, Loss: 1.065941572189331\n",
            "Epoch: 9, Loss: 1.037876844406128\n",
            "Epoch: 10, Loss: 1.0141593217849731\n",
            "Epoch: 11, Loss: 0.988396942615509\n",
            "Epoch: 12, Loss: 0.9641122817993164\n",
            "Epoch: 13, Loss: 0.941245973110199\n",
            "Epoch: 14, Loss: 0.925182044506073\n",
            "Epoch: 15, Loss: 0.9075639843940735\n",
            "Epoch: 16, Loss: 0.8882546424865723\n",
            "Epoch: 17, Loss: 0.8870198726654053\n",
            "Epoch: 18, Loss: 0.8822283744812012\n",
            "Epoch: 19, Loss: 0.8875859379768372\n",
            "Epoch: 20, Loss: 0.8873240947723389\n",
            "Epoch: 21, Loss: 0.8786548972129822\n",
            "Epoch: 22, Loss: 0.8609346151351929\n",
            "Epoch: 23, Loss: 0.8409835696220398\n",
            "Epoch: 24, Loss: 0.8295526504516602\n",
            "Epoch: 25, Loss: 0.8249633312225342\n",
            "Epoch: 26, Loss: 0.8181105852127075\n",
            "Epoch: 27, Loss: 0.8080682158470154\n",
            "Epoch: 28, Loss: 0.8062322735786438\n",
            "Epoch: 29, Loss: 0.8047101497650146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sb03NuI0llF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef1fb0fc-79ae-4f18-8c64-b68448b39f99"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints_k/ckpt_29'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqAVfd7BEjlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generating_language_model_k = build_model(vocab_size_k=vocab_size_k,rnn_units=rnn_units,batch_size=BATCH_SIZE)\n",
        "generating_language_model_k.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "generating_language_model_k.build(tf.TensorShape([1,None,vocab_size_k]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJEtZXT8EoeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "98cee2f6-ab05-4959-8dd4-21460b961f6e"
      },
      "source": [
        "generating_language_model_k.summary()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru_3 (GRU)                  multiple                  3391488   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  79950     \n",
            "=================================================================\n",
            "Total params: 3,471,438\n",
            "Trainable params: 3,471,438\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBH7CxOqEsnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "aa672ebc-df0b-4fe8-e3f6-8370e0d8d4c5"
      },
      "source": [
        "ch = 'L'  \n",
        "i = 0\n",
        "for key,val in vocab_k.items():\n",
        "  if key == ch:\n",
        "    i = val\n",
        "print(i)\n",
        "# convert to one hot vector\n",
        "input_ch = tf.one_hot(i,depth=vocab_size_k)\n",
        "input_ch = tf.expand_dims(tf.expand_dims(input_ch,axis=0),axis=0)\n",
        "print(input_ch)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41\n",
            "tf.Tensor(\n",
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0.]]], shape=(1, 1, 78), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZDCsDWUExLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_function(logits):\n",
        "  return tf.nn.softmax(axis=-1,logits=logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gD80pQrE2Gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "character_index_list_k = [i]\n",
        "index_list_k = list(range(vocab_size_k))\n",
        "generating_language_model_k.reset_states()\n",
        "for time_step in range(3000):\n",
        "\n",
        "    next_char = generating_language_model_k(input_ch)\n",
        "    softmax_char = softmax_function(next_char)\n",
        "    softmax_char = softmax_char.numpy()\n",
        "    index = np.random.choice(index_list_k,p = softmax_char.flatten())\n",
        "    character_index_list_k.append(index)\n",
        "\n",
        "    input_ch = tf.one_hot(index,depth=vocab_size_k)\n",
        "    input_ch = tf.expand_dims(tf.expand_dims(input_ch,axis=0),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex9DxpqQE8Vn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fdceef64-87a2-4cc9-fdfa-a29f61698013"
      },
      "source": [
        "seq_k = [ind_to_ch[ind] for ind in character_index_list_k]\n",
        "seq_k = [s.replace('</S>','\\n') for s in seq_k ]\n",
        "print(\"\".join(seq_k))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L﻿RD God preached the gospel unto\n",
            "thee, and to exacuse the eye shall suffer lobsentoisting one to\n",
            "another testament which I know not whether keep retinging them.\n",
            "\n",
            "\n",
            "elivery themselves to ankeph vioten in all things, ye and the flesh, saith the\n",
            "Lord, willing to consent to walk and the Father of our walks as\n",
            "sufferinged our offences for the king, and scrven lyan away as\n",
            "burdens; but we cann to repentance the saints.\n",
            "\n",
            "\n",
            "treace, the one of them in great power.\n",
            "\n",
            "\n",
            "nd forth like a disererce of grow saiveth the word of God, so for\n",
            "this people, and in the door of the deader and flesh that prophesied\n",
            "certain other of men for your souls.\n",
            "\n",
            "\n",
            "\n",
            "praise and confirmen it not.\n",
            "\n",
            "\n",
            "ranci and just consust themselves to her haughty, that we have\n",
            "believed in me.\n",
            "\n",
            "\n",
            "nded, \n",
            "eform he sweat by the roges of fie from his power, to profit them to\n",
            "spoiler, he confirmed the enemies of brass, even the head; every\n",
            "one that feared, and have place with him, and let them unto himself then\n",
            "he anouncence among kin doest the same partance and the cause by measure,\n",
            "and glory, the adfinitness of his neighbour; for the priest shall say,\n",
            "Where art those things which we hope: \n",
            "ut of the things that are of one place, so\n",
            "must in the temple of God beside me. And he did so neither iron by the will of\n",
            "God an hould and priest glad by the hailisg of the hore.\n",
            "\n",
            "\n",
            "\n",
            "something themselves.\n",
            "\n",
            "\n",
            "rew ye not the fore the few seven, and earth.\n",
            "\n",
            "\n",
            "ultlethers and palm trees.\n",
            "\n",
            "\n",
            "ne in partes therein, which thou hast seen vowition with the earth, the offering of the\n",
            "beast shall stend in the solemn affactors, and bring in\n",
            "other sackcloth: the people shall surely fiest up the sleep went and\n",
            "daughting Isknail.\n",
            "\n",
            "\n",
            "read for the lewer profity of the elders: \n",
            "e a quesengers, and\n",
            "have sent the good thereof.\n",
            "\n",
            "\n",
            "read yourselves together,\n",
            "full of coming of a gall of blood, lest he receive of the distress were of\n",
            "end.\n",
            "\n",
            "\n",
            "noughteth fever.\n",
            "\n",
            "\n",
            "e any thing according to his own well: and preaching all things that\n",
            "we shaul himself through Christ Jesus fur to mourning.\n",
            "\n",
            "\n",
            "etur in vain the spirits in the world.\n",
            "\n",
            "\n",
            "ndes drow thou well, when she had received of thanksgivoncus worketh\n",
            "in twoly: let not a wise seven liteled, that is the first past, that which have\n",
            "forbed the sun they abroad mercy, whether good to more then yet walking in\n",
            "the cold ye may destroy down from heaven.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "imscale things at all.\n",
            "\n",
            "\n",
            "e not the afflictions of the keepers of that workers holy, to do as I have\n",
            "been their sending the scriptures.\n",
            "\n",
            "\n",
            "ushel up from the world to come to houl, Through him the faithful in our ears,\n",
            "and beaith by faith.\n",
            "\n",
            "\n",
            "nd it therefore no companion there were followers.\n",
            "\n",
            "\n",
            "e unto your esses to take away the righteous thereup.\n",
            "\n",
            "\n",
            "e any olinu geasurestrrif in this good pertine any thing:\n",
            "\n",
            "\n",
            "e draw with that, heard and terrify report.\n",
            "\n",
            "\n",
            "ne in the flesh is now served from the sea; which is in their own\n",
            "consciences at these.\n",
            "\n",
            "\n",
            "non ssall walk through the inner poor.)  \n",
            "natiniss and other temperings; he hath\n",
            "beep resorred away as a parmet of the great disions.\n",
            "\n",
            "\n",
            "*\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}